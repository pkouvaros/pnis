
In this section we present an evaluation of the parameterised verification
procedure described in Section~\ref{sec:verification} for the guarding game
presented in Example~\ref{ex:agent-template}.

% The guarding game is an instance of a social dilemma game, a class of
% non-zero-sum MAS games characterised by intrinsic tension between individual
% and collective rationality \cite{VanlangeJPV13}. The guarding game simulates
% the fundamental forces of a \emph{collective risk dilemma (CRD)}, a type of
% public goods social dilemma game where a guaranteed tragedy of the commons
% \cite{Hardin68} is avoided by volunteered loss of utility by a population of
% agents. The stability of such a system can be curtailed by the immediate
% advantage of free riding if the agents favour a strategy which neglects the
% collective interests of maintaining the public good \cite{SantosP11}.
%
% In the guarding game, guarding can be considered equivalent to cooperation
% (acting in collective interest), and regenerating to defection (acting in
% selfish interest). The arbitrary selection of a subset of guards that volunteer
% is a necessary artefact of the swarm systems formalism, but does not affect the
% stability of the system negatively and allows us to reason about its
% existential properties, as the only event which would incur the tragedy of the
% commons is the lack of any volunteer guards in the first place.
%
% Many real systems exhibit CRD dynamics, one example being the collective
% maintenance of Earth's habitat by humanity \cite{Smirnov19}. Other examples
% include nuclear disarmament, responsible AI development, and financing public
% broadcast TV and radio entertainment. Furthermore, many of these can be
% directly or indirectly modelled as swarm systems, as we have done with guarding
% game. One example of this might be the huddling of emperor penguins to conserve
% energy and incubate eggs \cite{GilbertRLNA06}
% % The Emperor Penguin, Le Maho, 1977 during the
% arctic winter, the success of which depends on a slow rotation of members of
% the formation away from the chilling wind \cite{WatersBK12}.
%
% Given the existential nature of some challenges which come in the form of CRDs,
% it is unacceptable for us as a species to remain unprepared to reliably avoid
% their worst outcomes. Furthermore, many agents which play in CRD games may
% increasingly become autonomous given recent technological advances. The
% possibility of evading the tragedy of the commons through exporting some of our
% risky decision making to such autonomous agents then emerges if such agents can
% be verified to be more capable than we are of avoiding mutual defection as the
% number of agents grows. This work indicates that this possibility, as well as
% the possibility of better understanding systems which already evade the tragedy
% of the commmons - such as the aforementioned emperor penguins - may be worthy
% of pursuit.


The guarding game is an instance of a social dilemma game, a class of
MAS games characterised by tension between individual
and collective rationality \cite{VanlangeJPV13}. The guarding game simulates
the fundamental forces of a \emph{collective risk dilemma (CRD)}, a type of
social dilemma where a guaranteed tragedy of the commons
\cite{Hardin68} is avoided by volunteered loss of utility by a population of
agents. Such a system can be destabilised by the immediate advantage of
free riding if the agents favour a strategy which neglects the
collective interests of personal sacrifice \cite{SantosP11}.

In the guarding game, guarding can be considered equivalent to cooperation
(acting in collective interest), and regenerating to defection (acting in
selfish interest). The arbitrary selection of a subset of guards that volunteer
is a necessary artefact of the swarm systems formalism, but does not affect the
stability of the system negatively and allows us to reason about its
existential properties, as the only event which would incur the tragedy of the
commons is the lack of any volunteer guards in the first place.

Many real systems exhibit CRD dynamics, one example being the collective
maintenance of Earth's habitat by humanity \cite{Smirnov19}. Other examples
include nuclear disarmament, responsible AI development, and financing public
broadcast TV and radio entertainment. Furthermore, many of these can be
directly or indirectly modelled as swarm systems, as we have done with guarding
game. One example of this might be the huddling of emperor penguins to conserve
energy and incubate eggs \cite{GilbertRLNA06} during the
% The Emperor Penguin, Le Maho, 1977 
arctic winter, the success of which depends on a slow rotation of members of
the formation away from the chilling wind \cite{WatersBK12}.

Given the existential nature of some challenges which come in the form of CRDs,
it is unacceptable for us as a species to remain unprepared to reliably avoid
their worst outcomes. Furthermore, many agents which play in CRD games may
increasingly become autonomous given recent technological advances. The
possibility of evading the tragedy of the commons through exporting some of our
risky decision making to such autonomous agents then emerges if such agents can
be verified to be more capable than we are of avoiding mutual defection as the
number of agents grows. This work indicates that this possibility, as well as
the possibility of better understanding systems which already evade the tragedy
of the commmons - such as the aforementioned emperor penguins - may be worthy
of pursuit.

To train a neural observation function, we used deep Q-learning, a type of
reinforcement learning (RL) algorithm, by having a population of agents playing
the guarding game. The rewards were assigned in a way that
represented the tension between individual and collective interests. All agents
shared the same neural network, creating a system in which the agents were
learning to play against exact copies of themselves. We
used an experience replay buffer and a separate target network which
was updated only periodically, as per Mnih et al 2015, to stabilise
overestimation of the Q-values \cite{Mnih+15,HaaseltGS16}.
%
The resulting neural network's architecture had two hidden layers of four
neurons activated by ReLU.  The input layer consisted of a single neuron,
representing the normalised health points of the agent and the output layer
contained two neurons with linear activation functions, representing the
estimated Q-values of the two actions 'guard' and 'rest'.




%%%%
Given the neural network implementing the observation function, we 
implemented a template agent and a zero-one agent for the guarding game. We
then used the \venmas toolkit developed for verification of
NISs~\cite{Akintunde+20b} to verify $\bctl$ properties checking whether it is
possible for a colony of agents to survive after a number of time steps.
%
The experiments have been performed on a standard PC running Ubuntu 22.04 with
16GB RAM and processor Intel(R) Core i5-4460 CPU @ 3.20GHz. We relied on Gurobi
v10.0 to solve MILP \cite{Gurobi+16a}.

We assume that $M_h = 4$, $G_r = -2$, $R_r = 1$ and $U_r = -3$. 

Consider the guarding example PNIS $\pnis$ from Example~\ref{ex:pnis}, and recall that the proposition $\mathsf{a}$ labels all states with positive health (``alive'') and the proposition $\mathsf{d}$ all other states (``dead'').

\begin{table}
\centering
\begin{tabular}{c@{\qquad}rrrrr}
  \toprule
  k &\multicolumn{1}{c}{$n=2$} & \multicolumn{1}{c}{$n=3$} & \multicolumn{1}{c}{$n=4$} & \multicolumn{1}{c}{$n=5$} & \multicolumn{1}{c}{$n=6$}\\\midrule
  
  2 &              0.09s &     0.13s &   0.53s &    1.15s &              5.09s\\
  3 & \graycell    1.46s &     0.30s &   1.19s &    3.41s &             17.58s\\
  4 & \graycell    5.49s &     0.52s &   2.31s &   17.74s & \multicolumn{1}{c}{--}\\
  5 & \graycell   61.47s &   133.28s &   4.28s &   95.83s & \multicolumn{1}{c}{--}\\
  \bottomrule
\end{tabular}
\caption{ Verification times for $\msys n \models \varphi^k_E[m]$ for $m=2$ and
  various $k$ and $n$.  Grey cells indicate when the property was not
  satisfied.  Dashes indicate a 1 hour timeout.  }
  \label{tab:results-existential}
\end{table}

Let $m$ be the number of agent variables. We are interesting in verifying two
specifications:
\begin{inparaenum}[\it (i)]
\item existential property
  $\varphi^k_E = EX^k \bigwedge_{i=1}^m(\mathsf{a},v_i)$ for $v_i \in\atvar$
  expressing there is an evolution where at least $m$ agents are alive after
  $k$ time steps,
\item universal property $\varphi^k_A = AX^k \bigwedge_{i=1}^m(\mathsf{a},v_i)$
  expressing that in every possible evolution at least $m$ agents are alive
  after $k$ time steps.
\end{inparaenum}

For the existential property, we consider the case of $m=2$ and are interested
in finding the minimal number of agents in the colony that is required to
guarantee that at least two agents can stay alive for a given number of
steps. We vary the temporal depth $k$ of the property, from 2 till 6, and the
total number $n$ of agents from 2 to 6, and verify the property against
concrete NISs $\sys{n}$ composed of $n$ concrete agents.  The outcomes of the
verification queries are presented in Table~\ref{tab:results-existential}.
%
For temporal depth $k=2$, we have that
$\pnis\models \forall_{v_1,v_2}\varphi^k_E$. However, for $k\geq 3$,
$\pnis\not\models\forall_{v_1,v_2}\varphi^k_E$ as a system composed of two
agents only does not satisfy it, i.e.,
$\msys{2}\not\models\forall_{v_1,v_2}\varphi^k_E$. On the other hand, $n=3$ is
the emergence threshold for $\forall_{v_1,v_2}\varphi^k_E$ when $k\geq 3$. This
result is what we would expect from a cooperative template agent for the given
parameters of the guarding game. We can therefore conclude that the RL
procedure has been adequate for training altruistic agents.

For the universal property, we verified $\varphi^k_A[m]$ against $\masys{m}$
for temporal depth $k = 1,\dots,6$ and the number of agents $m=2,3$. The
verification results can be found in Table~\ref{tab:results-universal}.  When
we could get the result within the timeout, none of the properties has been
satisfied. This is a consequence of the semantics of our PNIS, where even when
the agents volunteer to guard, they can choose either to rest or to guard, and
so there is a path in the model where nobody is guarding and a path where
everybody is guarding, thus everyone's health deteriorates. This result
highlights the need for some arbitration in the social dilemma settings that
would follow some reasonable strategy instead of letting the system evolve due
to a chance.

\begin{table}
  \centering
  \begin{tabular}{crrrrrr}
    \toprule
    % $k$ & \multicolumn{1}{c}{$m = 2$} & \multicolumn{1}{c}{$m = 3$}\\
    % \midrule
    % 1 & \graycell        0.66s      & \graycell        1.75s\\
    % 2 & \graycell        3.60s      & \graycell       16.03s\\
    % 3 & \multicolumn{1}{c}{--}      & \graycell        0.00s\\
    % 4 & \graycell       16.90s      & \multicolumn{1}{c}{--}\\
    % 5 & \graycell       20.59s      & \multicolumn{1}{c}{--}\\
    % 6 & \multicolumn{1}{c}{--}      & \multicolumn{1}{c}{--}\\

    m & \multicolumn{1}{c}{$k = 1$} & \multicolumn{1}{c}{$k = 2$} & \multicolumn{1}{c}{$k = 3$} & \multicolumn{1}{c}{$k = 4$} & \multicolumn{1}{c}{$k = 5$} & \multicolumn{1}{c}{$k = 6$}\\
    \midrule
2 & \graycell    0.66s & \graycell    3.60s &            \multicolumn{1}{c}{--} & \graycell   16.90s & \graycell   20.59s &            \multicolumn{1}{c}{--}\\
3 & \graycell    1.75s & \graycell   16.03s & -- & \graycell 42.74s & \graycell 2196.70s & --\\
    \bottomrule
  \end{tabular}
  \caption{ Verification times for $\masys{m} \models \varphi^k_A[m]$ for various
    $m$ and $k$.  Grey cells indicate when the property was not satisfied.
    Dashes indicate a 1 hour timeout.  }
  \label{tab:results-universal}
\end{table}



%%% Local Variables:
%%% mode: latex
%%% fill-column: 79
%%% TeX-master: "../main"
%%% End:
