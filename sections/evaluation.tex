
In this section we present an evaluation of the parameterised verification
procedure described in Section~\ref{sec:verification} for the guarding game
presented in Example~\ref{ex:agent-template}.



The guarding game is an instance of a social dilemma game, a class of
non-zero-sum games characterised by intrinsic tension between individual and
collective rationality \cite{VanlangeJPV13}. The guarding game simulates the
fundamental forces of a \emph{collective risk dilemma (CRD)}, a type of public
goods game where a guaranteed tragedy of the commons \cite{Hardin68} is avoided
by volunteered loss of utility by a population of agents. The stability of a
such a system can be curtailed by the immediate advantage of free riding if the
agents favour a strategy which neglects the collective interests of maintaining
the public good \cite{SantosP11}. In the guarding game, guarding can be
considered equivalent to cooperation, and regenerating to defection.

Many real systems exhibit these dynamics, one example being the collective
maintenance of Earth's habitat by humanity \cite{Smirnov19}. Other examples
include nuclear disarmament, responsible AI development, and financing public
broadcast TV and radio entertainment. Furthermore, many of these can be
directly or indirectly modelled as swarm systems, as we have done with guarding
game. One example of this is the huddling of penguins in a cold environment
[]

Given the existential nature of some challenges which come in the form of CRDs,
it is unacceptable for us as a species to remain unprepared to reliably avoid
their worst outcomes. Furthermore, many agents which play in CRD games may
increasingly become autonomous given recent technological advances. The
possibility of evading the tragedy of the commons through exporting some of our
risky decision making to such autonomous agents then emerges if such agents can
be verified to be reliably capable of avoiding mutual defection. What follows
indicates that this possibility may be worthy of pursuit.

We used deep Q-learning, a type of algorithm used for reinforcement learning (RL) to train a neural observation function through a population of agents playing the guarding game. RL works by assigning rewards to state-action pairs, so that agents can learn the value of their behaviour; the rewards given to the network represented the tension between individual and collective interests. All agents shared the same neural network, creating a system in which the agents were learning to play against exact copies of themselves. 

The neural network's architecture had two hidden layers of four neurons each, all using ReLU activation functions for the benefit that the function thereby approximated would be piecewise linear [REF?]. The input layer consisted of a single neuron, representing the normalised HP of the agent and the output layer contained two neurons with linear activation functions, representing the estimated Q-values of the two actions. 

We trained the network according to the traditional Q-learning paradigm modified with an experience replay buffer and a separate target network which was updated only periodically, as per Mnih et al 2015, to stabilise overestimation of the Q-values [Human-level control through deep reinforcement learning, Mnih et al, 2015; Deep Reinforcement Learning with Double Q-learning, Hasselt, Guez and Silver, 2015]. 



Given the neural network implementing the observation function, we 
implemented a template agent and a zero-one agent for the guarding game. We
then used the \venmas toolkit developed for verification of
NISs~\cite{Akintunde+20b} to verify $\bctl$ properties checking whether it is
possible for a colony of agents to survive after a number of time steps.

We assume that $M_h = 4$, $G_r = -2$, $R_r = 1$ and $U_r = -3$. 

Assume the set $\atprop$ and the labelling function $\ell$ as in
Example~\ref{ex:pnis}. We evaluate the zero-one NIS
$\pnis_{zo} = \tuple{\set{1,\ldots,m,zo,e}, \globalinit{m}_{zo}, \valuation{m}_{zo}}$
against two specifications:
$\varphi^k_1 = EX^k \bigwedge_{i=1}^m(\mathsf{a},i)$,
expressing that there is a way to choose agents' actions from those available
to them so that all three agents are alive after $k$ time steps, and
$\varphi^k_2 = AX^k (\mathsf{a},1) \land (\mathsf{a},2) \land (\mathsf{a},3)$,
expressing that for all possible evolutions after $k$ time steps all three
agents are alive.

The experiments have been performed on a standard PC running Ubuntu 22.04 with
16GB RAM and processor Intel(R) Core i5-4460 CPU @ 3.20GHz. We relied on Gurobi
v9.5.1 to solve MILP \cite{Gurobi+16a}.



\begin{table}
\centering
\begin{tabular}{ccrcrcrc}
  \toprule
  & $k$ & \multicolumn{2}{c}{$m = 1$} & \multicolumn{2}{c}{$m = 2$} & \multicolumn{2}{c}{$m = 3$}  \\\midrule
  $\varphi_1^k$ & 1 &       0.27s & True    &       0.69s & True    &       1.87s & True   \\
 & 2 &       1.07s & True    &       3.30s & True    &      14.31s & True   \\
 & 3 &       1.84s & True    &       7.70s & True    &      28.91s & True   \\
 & 4 &      11.26s & False   &      38.14s & True    &     137.84s & True   \\
 & 5 &      10.84s & False   &      46.73s & True    &     237.68s & True   \\
 & 6 &      14.45s & False   &      31.41s & True    &     339.65s & True   \\


  
  \midrule
  $\varphi_2^k$ & 1 &    0.65s & False   &     1.79s & False  \\
               & 2 &     3.32s & False   &    14.67s & False  \\
               & 3 &  3601.62s & Timeout &     0.00s & False  \\
               & 4 &    17.12s & False   &     0.00s & False  \\
               & 5 &    20.50s & False   &     0.00s & False  \\
               & 6 &  3603.07s & Timeout &     0.00s & False  \\


\bottomrule
\end{tabular}
  % \caption{ Verification times for \guard{m} against properties $\varphi_{1}^{k}$
  %   and $\varphi_{2}^{k}$ for various $k$ and $h_\init$.  Grey cells indicate a
  %   % \texttt{\greyc} result, otherwise a \texttt{True} result was obtained.
  %   % Dashes indicate a \timeout hour timeout.
  % }
  \label{tab:results}
\end{table}



%%% Local Variables:
%%% mode: latex
%%% fill-column: 79
%%% TeX-master: "../main"
%%% End:
